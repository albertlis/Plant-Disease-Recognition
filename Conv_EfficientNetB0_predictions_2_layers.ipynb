{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv EfficientNetB0 predictions 2 layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNaqr6W9SSEc0w9/LOazSli",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertlis/Plant-Disease-Recognition/blob/main/Conv_EfficientNetB0_predictions_2_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pce5jvbclOPW",
        "outputId": "f0690a79-4c8a-440a-84d9-1d88b6cd2855"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "drive.mount(\"/content/drive\")\r\n",
        "!unzip -q /content/drive/My\\ Drive/PDR/Data/original.zip -d /content\r\n",
        "!rm -r sample_data"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AlVLxvHpfhS"
      },
      "source": [
        "Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3IjN0-opjqy"
      },
      "source": [
        "img_shape = (224, 224, 3)\r\n",
        "e_net_out_shape = (7, 7, 1280)\r\n",
        "nr_of_imgs = 49940\r\n",
        "nr_of_val_imgs = 3862\r\n",
        "batch_size = 64\r\n",
        "nr_of_classes = 39\r\n",
        "train_path = './original/train'\r\n",
        "val_path = './original/val'\r\n",
        "model_path = '/content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5'\r\n",
        "monitor = 'val_acc'\r\n",
        "train_imgs_filename, train_labels_filename = 'imgs.npy', 'labels.npy'\r\n",
        "val_imgs_filename, val_labels_filename = 'val_imgs.npy', 'val_labels.npy'\r\n",
        "preds_dtype = 'float16'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3id7bkG0UYXP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po0ue1sBoq_E"
      },
      "source": [
        "Get predictions from Convolutional part to fast train classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjlpB0lflqj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f22ea5b-d9bd-4f02-930d-832b78052718"
      },
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "e_net = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=img_shape)\r\n",
        "\r\n",
        "def make_conv_predictions():\r\n",
        "    gen = ImageDataGenerator(rotation_range=45, horizontal_flip=True, vertical_flip=True, rescale=1/255)\r\n",
        "    datagen = gen.flow_from_directory(train_path, target_size=img_shape[:2], batch_size=batch_size, class_mode='categorical')\r\n",
        "    val_datagen = gen.flow_from_directory(val_path, target_size=img_shape[:2], batch_size=batch_size, class_mode='categorical')\r\n",
        "\r\n",
        "    imgs = np.lib.format.open_memmap(train_imgs_filename, dtype=preds_dtype, mode='w+', shape=((nr_of_imgs,) + e_net_out_shape))\r\n",
        "    labels = np.lib.format.open_memmap(train_labels_filename, dtype='uint8', mode='w+', shape=(nr_of_imgs, nr_of_classes))\r\n",
        "    val_imgs = np.lib.format.open_memmap(val_imgs_filename, dtype=preds_dtype, mode='w+', shape=((nr_of_val_imgs,) + e_net_out_shape))\r\n",
        "    val_labels = np.lib.format.open_memmap(val_labels_filename, dtype='uint8', mode='w+', shape=(nr_of_val_imgs, nr_of_classes))\r\n",
        "\r\n",
        "    for i, (imgs_batch, labels_batch) in enumerate(datagen):\r\n",
        "        count = i * batch_size\r\n",
        "        line = ' '\r\n",
        "        if not i % 20 and i != 0:\r\n",
        "            line = '\\n'\r\n",
        "        print(f'%5d{line}' %(count), end='')\r\n",
        "        if count > nr_of_imgs:\r\n",
        "            break\r\n",
        "        predictions = e_net.predict(imgs_batch)\r\n",
        "        imgs[count : count + batch_size] = predictions\r\n",
        "        labels[count : count + batch_size] = labels_batch\r\n",
        "    print()\r\n",
        "\r\n",
        "    for i, (imgs_batch, labels_batch) in enumerate(val_datagen):\r\n",
        "        count = i * batch_size\r\n",
        "        line = ' '\r\n",
        "        if not i % 20 and i != 0:\r\n",
        "            line = '\\n'\r\n",
        "        print(f'%5d{line}' %(count), end='')\r\n",
        "        if count > nr_of_val_imgs:\r\n",
        "            break\r\n",
        "        predictions = e_net.predict(imgs_batch)\r\n",
        "        val_imgs[count : count + batch_size] = predictions\r\n",
        "        val_labels[count : count + batch_size] = labels_batch\r\n",
        "    print()\r\n",
        "    \r\n",
        "make_conv_predictions()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "Found 49940 images belonging to 39 classes.\n",
            "Found 3862 images belonging to 39 classes.\n",
            "    0    64   128   192   256   320   384   448   512   576   640   704   768   832   896   960  1024  1088  1152  1216  1280\n",
            " 1344  1408  1472  1536  1600  1664  1728  1792  1856  1920  1984  2048  2112  2176  2240  2304  2368  2432  2496  2560\n",
            " 2624  2688  2752  2816  2880  2944  3008  3072  3136  3200  3264  3328  3392  3456  3520  3584  3648  3712  3776  3840\n",
            " 3904  3968  4032  4096  4160  4224  4288  4352  4416  4480  4544  4608  4672  4736  4800  4864  4928  4992  5056  5120\n",
            " 5184  5248  5312  5376  5440  5504  5568  5632  5696  5760  5824  5888  5952  6016  6080  6144  6208  6272  6336  6400\n",
            " 6464  6528  6592  6656  6720  6784  6848  6912  6976  7040  7104  7168  7232  7296  7360  7424  7488  7552  7616  7680\n",
            " 7744  7808  7872  7936  8000  8064  8128  8192  8256  8320  8384  8448  8512  8576  8640  8704  8768  8832  8896  8960\n",
            " 9024  9088  9152  9216  9280  9344  9408  9472  9536  9600  9664  9728  9792  9856  9920  9984 10048 10112 10176 10240\n",
            "10304 10368 10432 10496 10560 10624 10688 10752 10816 10880 10944 11008 11072 11136 11200 11264 11328 11392 11456 11520\n",
            "11584 11648 11712 11776 11840 11904 11968 12032 12096 12160 12224 12288 12352 12416 12480 12544 12608 12672 12736 12800\n",
            "12864 12928 12992 13056 13120 13184 13248 13312 13376 13440 13504 13568 13632 13696 13760 13824 13888 13952 14016 14080\n",
            "14144 14208 14272 14336 14400 14464 14528 14592 14656 14720 14784 14848 14912 14976 15040 15104 15168 15232 15296 15360\n",
            "15424 15488 15552 15616 15680 15744 15808 15872 15936 16000 16064 16128 16192 16256 16320 16384 16448 16512 16576 16640\n",
            "16704 16768 16832 16896 16960 17024 17088 17152 17216 17280 17344 17408 17472 17536 17600 17664 17728 17792 17856 17920\n",
            "17984 18048 18112 18176 18240 18304 18368 18432 18496 18560 18624 18688 18752 18816 18880 18944 19008 19072 19136 19200\n",
            "19264 19328 19392 19456 19520 19584 19648 19712 19776 19840 19904 19968 20032 20096 20160 20224 20288 20352 20416 20480\n",
            "20544 20608 20672 20736 20800 20864 20928 20992 21056 21120 21184 21248 21312 21376 21440 21504 21568 21632 21696 21760\n",
            "21824 21888 21952 22016 22080 22144 22208 22272 22336 22400 22464 22528 22592 22656 22720 22784 22848 22912 22976 23040\n",
            "23104 23168 23232 23296 23360 23424 23488 23552 23616 23680 23744 23808 23872 23936 24000 24064 24128 24192 24256 24320\n",
            "24384 24448 24512 24576 24640 24704 24768 24832 24896 24960 25024 25088 25152 25216 25280 25344 25408 25472 25536 25600\n",
            "25664 25728 25792 25856 25920 25984 26048 26112 26176 26240 26304 26368 26432 26496 26560 26624 26688 26752 26816 26880\n",
            "26944 27008 27072 27136 27200 27264 27328 27392 27456 27520 27584 27648 27712 27776 27840 27904 27968 28032 28096 28160\n",
            "28224 28288 28352 28416 28480 28544 28608 28672 28736 28800 28864 28928 28992 29056 29120 29184 29248 29312 29376 29440\n",
            "29504 29568 29632 29696 29760 29824 29888 29952 30016 30080 30144 30208 30272 30336 30400 30464 30528 30592 30656 30720\n",
            "30784 30848 30912 30976 31040 31104 31168 31232 31296 31360 31424 31488 31552 31616 31680 31744 31808 31872 31936 32000\n",
            "32064 32128 32192 32256 32320 32384 32448 32512 32576 32640 32704 32768 32832 32896 32960 33024 33088 33152 33216 33280\n",
            "33344 33408 33472 33536 33600 33664 33728 33792 33856 33920 33984 34048 34112 34176 34240 34304 34368 34432 34496 34560\n",
            "34624 34688 34752 34816 34880 34944 35008 35072 35136 35200 35264 35328 35392 35456 35520 35584 35648 35712 35776 35840\n",
            "35904 35968 36032 36096 36160 36224 36288 36352 36416 36480 36544 36608 36672 36736 36800 36864 36928 36992 37056 37120\n",
            "37184 37248 37312 37376 37440 37504 37568 37632 37696 37760 37824 37888 37952 38016 38080 38144 38208 38272 38336 38400\n",
            "38464 38528 38592 38656 38720 38784 38848 38912 38976 39040 39104 39168 39232 39296 39360 39424 39488 39552 39616 39680\n",
            "39744 39808 39872 39936 40000 40064 40128 40192 40256 40320 40384 40448 40512 40576 40640 40704 40768 40832 40896 40960\n",
            "41024 41088 41152 41216 41280 41344 41408 41472 41536 41600 41664 41728 41792 41856 41920 41984 42048 42112 42176 42240\n",
            "42304 42368 42432 42496 42560 42624 42688 42752 42816 42880 42944 43008 43072 43136 43200 43264 43328 43392 43456 43520\n",
            "43584 43648 43712 43776 43840 43904 43968 44032 44096 44160 44224 44288 44352 44416 44480 44544 44608 44672 44736 44800\n",
            "44864 44928 44992 45056 45120 45184 45248 45312 45376 45440 45504 45568 45632 45696 45760 45824 45888 45952 46016 46080\n",
            "46144 46208 46272 46336 46400 46464 46528 46592 46656 46720 46784 46848 46912 46976 47040 47104 47168 47232 47296 47360\n",
            "47424 47488 47552 47616 47680 47744 47808 47872 47936 48000 48064 48128 48192 48256 48320 48384 48448 48512 48576 48640\n",
            "48704 48768 48832 48896 48960 49024 49088 49152 49216 49280 49344 49408 49472 49536 49600 49664 49728 49792 49856 49920\n",
            "49984 \n",
            "    0    64   128   192   256   320   384   448   512   576   640   704   768   832   896   960  1024  1088  1152  1216  1280\n",
            " 1344  1408  1472  1536  1600  1664  1728  1792  1856  1920  1984  2048  2112  2176  2240  2304  2368  2432  2496  2560\n",
            " 2624  2688  2752  2816  2880  2944  3008  3072  3136  3200  3264  3328  3392  3456  3520  3584  3648  3712  3776  3840\n",
            " 3904 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZyghXul69Ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd87ccb1-27a0-4791-b0ea-f2c8f594a5fd"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\r\n",
        "from tensorflow.keras.layers import Dense, Flatten, InputLayer, BatchNormalization, Dropout\r\n",
        "import tensorflow.keras.callbacks as clb\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.constraints import max_norm\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "norm = max_norm(4)\r\n",
        "\r\n",
        "def build_model():\r\n",
        "    model = Sequential()\r\n",
        "    \r\n",
        "    model.add(InputLayer(input_shape=e_net_out_shape))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(659, kernel_constraint=norm, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(39, kernel_constraint=norm, activation='softmax'))\r\n",
        "\r\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "    return model\r\n",
        "\r\n",
        "def np_array_memmap_gen(feat_path, label_path, batch_size=128, shuffle_array=True):\r\n",
        "    while 1:\r\n",
        "        x = np.load(feat_path, mmap_mode='r')\r\n",
        "        y = np.load(label_path, mmap_mode='r')\r\n",
        "        lst = [i for i in range(x.shape[0])]\r\n",
        "\r\n",
        "        if shuffle_array:\r\n",
        "            random.shuffle(lst)\r\n",
        "\r\n",
        "        iters = len(lst) // batch_size + 1\r\n",
        "\r\n",
        "        for i in range(iters):\r\n",
        "            start = i * batch_size\r\n",
        "            end = (i + 1) * batch_size\r\n",
        "            yield (x[lst[start : end]], y[lst[start : end]])\r\n",
        "\r\n",
        "callbacks = [\r\n",
        "            clb.ReduceLROnPlateau(monitor=monitor, factor=0.1, min_lr=1e-7, patience=3, verbose=1),\r\n",
        "            clb.EarlyStopping(monitor=monitor, patience=7, verbose=1),\r\n",
        "            clb.ModelCheckpoint(monitor=monitor, filepath=model_path, save_best_only=True, verbose=1)\r\n",
        "            ]\r\n",
        "\r\n",
        "train_gen = np_array_memmap_gen(train_imgs_filename, train_labels_filename, batch_size=batch_size)\r\n",
        "val_gen = np_array_memmap_gen(val_imgs_filename, val_labels_filename, batch_size=batch_size)\r\n",
        "\r\n",
        "train_steps = nr_of_imgs // batch_size + 1\r\n",
        "val_steps = nr_of_val_imgs // batch_size + 1\r\n",
        "\r\n",
        "# model = build_model()\r\n",
        "\r\n",
        "model = load_model(model_path)\r\n",
        "\r\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "\r\n",
        "history = model.fit(train_gen, epochs=100, validation_data=val_gen, callbacks=callbacks, verbose=1, \r\n",
        "                    steps_per_epoch=train_steps, validation_steps=val_steps)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "781/781 [==============================] - 195s 246ms/step - loss: 1.4347 - acc: 0.5740 - val_loss: 3.7302 - val_acc: 0.2452\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.24521, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 2/100\n",
            "781/781 [==============================] - 132s 170ms/step - loss: 1.4971 - acc: 0.5614 - val_loss: 1.9013 - val_acc: 0.4653\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.24521 to 0.46530, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 3/100\n",
            "781/781 [==============================] - 94s 121ms/step - loss: 1.5032 - acc: 0.5582 - val_loss: 2.2301 - val_acc: 0.3809\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.46530\n",
            "Epoch 4/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.5171 - acc: 0.5528 - val_loss: 2.2870 - val_acc: 0.4117\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.46530\n",
            "Epoch 5/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.5525 - acc: 0.5422 - val_loss: 2.1447 - val_acc: 0.4531\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.46530\n",
            "Epoch 6/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.3976 - acc: 0.5892 - val_loss: 1.1212 - val_acc: 0.6836\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.46530 to 0.68358, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 7/100\n",
            "781/781 [==============================] - 25s 32ms/step - loss: 1.3026 - acc: 0.6187 - val_loss: 1.0931 - val_acc: 0.7035\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.68358 to 0.70352, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 8/100\n",
            "781/781 [==============================] - 21s 27ms/step - loss: 1.2657 - acc: 0.6278 - val_loss: 1.1049 - val_acc: 0.6888\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.70352\n",
            "Epoch 9/100\n",
            "781/781 [==============================] - 20s 25ms/step - loss: 1.2495 - acc: 0.6270 - val_loss: 1.1146 - val_acc: 0.7051\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.70352 to 0.70508, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 10/100\n",
            "781/781 [==============================] - 21s 26ms/step - loss: 1.2103 - acc: 0.6430 - val_loss: 1.0751 - val_acc: 0.6877\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.70508\n",
            "Epoch 11/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.1922 - acc: 0.6464 - val_loss: 1.0348 - val_acc: 0.7126\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.70508 to 0.71258, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 12/100\n",
            "781/781 [==============================] - 21s 27ms/step - loss: 1.1746 - acc: 0.6562 - val_loss: 1.0300 - val_acc: 0.7147\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.71258 to 0.71466, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 13/100\n",
            "781/781 [==============================] - 21s 27ms/step - loss: 1.1552 - acc: 0.6611 - val_loss: 1.0127 - val_acc: 0.7180\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.71466 to 0.71802, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 14/100\n",
            "781/781 [==============================] - 21s 27ms/step - loss: 1.1543 - acc: 0.6583 - val_loss: 1.0247 - val_acc: 0.7172\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.71802\n",
            "Epoch 15/100\n",
            "781/781 [==============================] - 20s 25ms/step - loss: 1.1449 - acc: 0.6634 - val_loss: 1.0347 - val_acc: 0.7095\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.71802\n",
            "Epoch 16/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.1350 - acc: 0.6664 - val_loss: 0.9636 - val_acc: 0.7292\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.71802 to 0.72916, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 17/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.1260 - acc: 0.6690 - val_loss: 0.9829 - val_acc: 0.7393\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.72916 to 0.73925, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 18/100\n",
            "781/781 [==============================] - 21s 26ms/step - loss: 1.1094 - acc: 0.6744 - val_loss: 0.9815 - val_acc: 0.7284\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.73925\n",
            "Epoch 19/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.1015 - acc: 0.6742 - val_loss: 0.9988 - val_acc: 0.7356\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.73925\n",
            "Epoch 20/100\n",
            "781/781 [==============================] - 20s 25ms/step - loss: 1.0798 - acc: 0.6819 - val_loss: 0.9574 - val_acc: 0.7248\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.73925\n",
            "Epoch 21/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0552 - acc: 0.6877 - val_loss: 0.9846 - val_acc: 0.7545\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.73925 to 0.75453, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 22/100\n",
            "781/781 [==============================] - 21s 27ms/step - loss: 1.0484 - acc: 0.6907 - val_loss: 0.9324 - val_acc: 0.7455\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.75453\n",
            "Epoch 23/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0559 - acc: 0.6865 - val_loss: 0.9114 - val_acc: 0.7545\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.75453\n",
            "Epoch 24/100\n",
            "781/781 [==============================] - 20s 25ms/step - loss: 1.0384 - acc: 0.6932 - val_loss: 0.9433 - val_acc: 0.7530\n",
            "\n",
            "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.75453\n",
            "Epoch 25/100\n",
            "781/781 [==============================] - 20s 25ms/step - loss: 1.0455 - acc: 0.6877 - val_loss: 0.8732 - val_acc: 0.7587\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.75453 to 0.75867, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 26/100\n",
            "781/781 [==============================] - 21s 27ms/step - loss: 1.0462 - acc: 0.6944 - val_loss: 0.9282 - val_acc: 0.7538\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.75867\n",
            "Epoch 27/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0425 - acc: 0.6910 - val_loss: 0.9464 - val_acc: 0.7499\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.75867\n",
            "Epoch 28/100\n",
            "781/781 [==============================] - 20s 25ms/step - loss: 1.0307 - acc: 0.6953 - val_loss: 0.9337 - val_acc: 0.7626\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.75867 to 0.76256, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 29/100\n",
            "781/781 [==============================] - 21s 27ms/step - loss: 1.0469 - acc: 0.6922 - val_loss: 0.8632 - val_acc: 0.7499\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.76256\n",
            "Epoch 30/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0363 - acc: 0.6957 - val_loss: 0.8742 - val_acc: 0.7550\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.76256\n",
            "Epoch 31/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0424 - acc: 0.6917 - val_loss: 0.9708 - val_acc: 0.7450\n",
            "\n",
            "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.76256\n",
            "Epoch 32/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0267 - acc: 0.6966 - val_loss: 0.8835 - val_acc: 0.7566\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.76256\n",
            "Epoch 33/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0388 - acc: 0.6919 - val_loss: 0.9530 - val_acc: 0.7527\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.76256\n",
            "Epoch 34/100\n",
            "781/781 [==============================] - 20s 25ms/step - loss: 1.0333 - acc: 0.6945 - val_loss: 0.9060 - val_acc: 0.7470\n",
            "\n",
            "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.76256\n",
            "Epoch 35/100\n",
            "781/781 [==============================] - 20s 26ms/step - loss: 1.0476 - acc: 0.6907 - val_loss: 0.9196 - val_acc: 0.7595\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.76256\n",
            "Epoch 00035: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}