{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Conv EfficientNetB0 predictions 2 layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYSV6vnviM95N9e5ZZS9o2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertlis/Plant-Disease-Recognition/blob/main/notebooks/classifier/Conv_EfficientNetB0_predictions_2_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pce5jvbclOPW",
        "outputId": "0e39a878-9911-456b-c98e-aa88b041ef13"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "drive.mount(\"/content/drive\")\r\n",
        "!unzip -q /content/drive/My\\ Drive/PDR/Data/original.zip -d /content\r\n",
        "!rm -r sample_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AlVLxvHpfhS"
      },
      "source": [
        "Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3IjN0-opjqy"
      },
      "source": [
        "img_shape = (224, 224, 3)\r\n",
        "e_net_out_shape = (7, 7, 1280)\r\n",
        "nr_of_imgs = 49940\r\n",
        "nr_of_val_imgs = 3862\r\n",
        "batch_size = 64\r\n",
        "nr_of_classes = 39\r\n",
        "train_path = './original/train'\r\n",
        "val_path = './original/val'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3id7bkG0UYXP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po0ue1sBoq_E"
      },
      "source": [
        "Get predictions from Convolutional part to fast train classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjlpB0lflqj6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ef176e-aea4-4212-e4a8-db4a0356537c"
      },
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "e_net = EfficientNetB0(include_top=False, weights=\"imagenet\", input_shape=img_shape)\r\n",
        "\r\n",
        "def make_conv_predictions():\r\n",
        "    gen = ImageDataGenerator(rotation_range=45, horizontal_flip=True, vertical_flip=True, rescale=1/255)\r\n",
        "    datagen = gen.flow_from_directory(train_path, target_size=img_shape[:2], batch_size=batch_size, class_mode='categorical')\r\n",
        "    val_datagen = gen.flow_from_directory(val_path, target_size=img_shape[:2], batch_size=batch_size, class_mode='categorical')\r\n",
        "\r\n",
        "    imgs = np.lib.format.open_memmap('imgs.npy', dtype='float32', mode='w+', shape=((nr_of_imgs,) + e_net_out_shape))\r\n",
        "    labels = np.lib.format.open_memmap('labels.npy', dtype='uint8', mode='w+', shape=(nr_of_imgs, nr_of_classes))\r\n",
        "    val_imgs = np.lib.format.open_memmap('val_imgs.npy', dtype='float32', mode='w+', shape=((nr_of_val_imgs,) + e_net_out_shape))\r\n",
        "    val_labels = np.lib.format.open_memmap('val_labels.npy', dtype='uint8', mode='w+', shape=(nr_of_val_imgs, nr_of_classes))\r\n",
        "\r\n",
        "    for i, (imgs_batch, labels_batch) in enumerate(datagen):\r\n",
        "        count = i * batch_size\r\n",
        "        line = ' '\r\n",
        "        if not i % 20 and i != 0:\r\n",
        "            line = '\\n'\r\n",
        "        print(f'%5d{line}' %(count), end='')\r\n",
        "        if count > nr_of_imgs:\r\n",
        "            break\r\n",
        "        predictions = e_net.predict(imgs_batch)\r\n",
        "        imgs[count : count + batch_size] = predictions\r\n",
        "        labels[count : count + batch_size] = labels_batch\r\n",
        "    print()\r\n",
        "\r\n",
        "    for i, (imgs_batch, labels_batch) in enumerate(val_datagen):\r\n",
        "        count = i * batch_size\r\n",
        "        line = ' '\r\n",
        "        if not i % 20 and i != 0:\r\n",
        "            line = '\\n'\r\n",
        "        print(f'%5d{line}' %(count), end='')\r\n",
        "        if count > nr_of_val_imgs:\r\n",
        "            break\r\n",
        "        predictions = e_net.predict(imgs_batch)\r\n",
        "        val_imgs[count : count + batch_size] = predictions\r\n",
        "        val_labels[count : count + batch_size] = labels_batch\r\n",
        "    print()\r\n",
        "    \r\n",
        "make_conv_predictions()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
            "16711680/16705208 [==============================] - 0s 0us/step\n",
            "Found 49940 images belonging to 39 classes.\n",
            "Found 3862 images belonging to 39 classes.\n",
            "    0    16    32    48    64    80    96   112   128   144   160   176   192   208   224   240   256   272   288   304   320\n",
            "  336   352   368   384   400   416   432   448   464   480   496   512   528   544   560   576   592   608   624   640\n",
            "  656   672   688   704   720   736   752   768   784   800   816   832   848   864   880   896   912   928   944   960\n",
            "  976   992  1008  1024  1040  1056  1072  1088  1104  1120  1136  1152  1168  1184  1200  1216  1232  1248  1264  1280\n",
            " 1296  1312  1328  1344  1360  1376  1392  1408  1424  1440  1456  1472  1488  1504  1520  1536  1552  1568  1584  1600\n",
            " 1616  1632  1648  1664  1680  1696  1712  1728  1744  1760  1776  1792  1808  1824  1840  1856  1872  1888  1904  1920\n",
            " 1936  1952  1968  1984  2000  2016  2032  2048  2064  2080  2096  2112  2128  2144  2160  2176  2192  2208  2224  2240\n",
            " 2256  2272  2288  2304  2320  2336  2352  2368  2384  2400  2416  2432  2448  2464  2480  2496  2512  2528  2544  2560\n",
            " 2576  2592  2608  2624  2640  2656  2672  2688  2704  2720  2736  2752  2768  2784  2800  2816  2832  2848  2864  2880\n",
            " 2896  2912  2928  2944  2960  2976  2992  3008  3024  3040  3056  3072  3088  3104  3120  3136  3152  3168  3184  3200\n",
            " 3216  3232  3248  3264  3280  3296  3312  3328  3344  3360  3376  3392  3408  3424  3440  3456  3472  3488  3504  3520\n",
            " 3536  3552  3568  3584  3600  3616  3632  3648  3664  3680  3696  3712  3728  3744  3760  3776  3792  3808  3824  3840\n",
            " 3856  3872  3888  3904  3920  3936  3952  3968  3984  4000  4016  4032  4048  4064  4080  4096  4112  4128  4144  4160\n",
            " 4176  4192  4208  4224  4240  4256  4272  4288  4304  4320  4336  4352  4368  4384  4400  4416  4432  4448  4464  4480\n",
            " 4496  4512  4528  4544  4560  4576  4592  4608  4624  4640  4656  4672  4688  4704  4720  4736  4752  4768  4784  4800\n",
            " 4816  4832  4848  4864  4880  4896  4912  4928  4944  4960  4976  4992  5008  5024  5040  5056  5072  5088  5104  5120\n",
            " 5136  5152  5168  5184  5200  5216  5232  5248  5264  5280  5296  5312  5328  5344  5360  5376  5392  5408  5424  5440\n",
            " 5456  5472  5488  5504  5520  5536  5552  5568  5584  5600  5616  5632  5648  5664  5680  5696  5712  5728  5744  5760\n",
            " 5776  5792  5808  5824  5840  5856  5872  5888  5904  5920  5936  5952  5968  5984  6000  6016  6032  6048  6064  6080\n",
            " 6096  6112  6128  6144  6160  6176  6192  6208  6224  6240  6256  6272  6288  6304  6320  6336  6352  6368  6384  6400\n",
            " 6416  6432  6448  6464  6480  6496  6512  6528  6544  6560  6576  6592  6608  6624  6640  6656  6672  6688  6704  6720\n",
            " 6736  6752  6768  6784  6800  6816  6832  6848  6864  6880  6896  6912  6928  6944  6960  6976  6992  7008  7024  7040\n",
            " 7056  7072  7088  7104  7120  7136  7152  7168  7184  7200  7216  7232  7248  7264  7280  7296  7312  7328  7344  7360\n",
            " 7376  7392  7408  7424  7440  7456  7472  7488  7504  7520  7536  7552  7568  7584  7600  7616  7632  7648  7664  7680\n",
            " 7696  7712  7728  7744  7760  7776  7792  7808  7824  7840  7856  7872  7888  7904  7920  7936  7952  7968  7984  8000\n",
            " 8016  8032  8048  8064  8080  8096  8112  8128  8144  8160  8176  8192  8208  8224  8240  8256  8272  8288  8304  8320\n",
            " 8336  8352  8368  8384  8400  8416  8432  8448  8464  8480  8496  8512  8528  8544  8560  8576  8592  8608  8624  8640\n",
            " 8656  8672  8688  8704  8720  8736  8752  8768  8784  8800  8816  8832  8848  8864  8880  8896  8912  8928  8944  8960\n",
            " 8976  8992  9008  9024  9040  9056  9072  9088  9104  9120  9136  9152  9168  9184  9200  9216  9232  9248  9264  9280\n",
            " 9296  9312  9328  9344  9360  9376  9392  9408  9424  9440  9456  9472  9488  9504  9520  9536  9552  9568  9584  9600\n",
            " 9616  9632  9648  9664  9680  9696  9712  9728  9744  9760  9776  9792  9808  9824  9840  9856  9872  9888  9904  9920\n",
            " 9936  9952  9968  9984 10000 10016 10032 10048 10064 10080 10096 10112 10128 10144 10160 10176 10192 10208 10224 10240\n",
            "10256 10272 10288 10304 10320 10336 10352 10368 10384 10400 10416 10432 10448 10464 10480 10496 10512 10528 10544 10560\n",
            "10576 10592 10608 10624 10640 10656 10672 10688 10704 10720 10736 10752 10768 10784 10800 10816 10832 10848 10864 10880\n",
            "10896 10912 10928 10944 10960 10976 10992 11008 11024 11040 11056 11072 11088 11104 11120 11136 11152 11168 11184 11200\n",
            "11216 11232 11248 11264 11280 11296 11312 11328 11344 11360 11376 11392 11408 11424 11440 11456 11472 11488 11504 11520\n",
            "11536 11552 11568 11584 11600 11616 11632 11648 11664 11680 11696 11712 11728 11744 11760 11776 11792 11808 11824 11840\n",
            "11856 11872 11888 11904 11920 11936 11952 11968 11984 12000 12016 12032 12048 12064 12080 12096 12112 12128 12144 12160\n",
            "12176 12192 12208 12224 12240 12256 12272 12288 12304 12320 12336 12352 12368 12384 12400 12416 12432 12448 12464 12480\n",
            "12496 12512 12528 12544 12560 12576 12592 12608 12624 12640 12656 12672 12688 12704 12720 12736 12752 12768 12784 12800\n",
            "12816 12832 12848 12864 12880 12896 12912 12928 12944 12960 12976 12992 13008 13024 13040 13056 13072 13088 13104 13120\n",
            "13136 13152 13168 13184 13200 13216 13232 13248 13264 13280 13296 13312 13328 13344 13360 13376 13392 13408 13424 13440\n",
            "13456 13472 13488 13504 13520 13536 13552 13568 13584 13600 13616 13632 13648 13664 13680 13696 13712 13728 13744 13760\n",
            "13776 13792 13808 13824 13840 13856 13872 13888 13904 13920 13936 13952 13968 13984 14000 14016 14032 14048 14064 14080\n",
            "14096 14112 14128 14144 14160 14176 14192 14208 14224 14240 14256 14272 14288 14304 14320 14336 14352 14368 14384 14400\n",
            "14416 14432 14448 14464 14480 14496 14512 14528 14544 14560 14576 14592 14608 14624 14640 14656 14672 14688 14704 14720\n",
            "14736 14752 14768 14784 14800 14816 14832 14848 14864 14880 14896 14912 14928 14944 14960 14976 14992 15008 15024 15040\n",
            "15056 15072 15088 15104 15120 15136 15152 15168 15184 15200 15216 15232 15248 15264 15280 15296 15312 15328 15344 15360\n",
            "15376 15392 15408 15424 15440 15456 15472 15488 15504 15520 15536 15552 15568 15584 15600 15616 15632 15648 15664 15680\n",
            "15696 15712 15728 15744 15760 15776 15792 15808 15824 15840 15856 15872 15888 15904 15920 15936 15952 15968 15984 16000\n",
            "16016 16032 16048 16064 16080 16096 16112 16128 16144 16160 16176 16192 16208 16224 16240 16256 16272 16288 16304 16320\n",
            "16336 16352 16368 16384 16400 16416 16432 16448 16464 16480 16496 16512 16528 16544 16560 16576 16592 16608 16624 16640\n",
            "16656 16672 16688 16704 16720 16736 16752 16768 16784 16800 16816 16832 16848 16864 16880 16896 16912 16928 16944 16960\n",
            "16976 16992 17008 17024 17040 17056 17072 17088 17104 17120 17136 17152 17168 17184 17200 17216 17232 17248 17264 17280\n",
            "17296 17312 17328 17344 17360 17376 17392 17408 17424 17440 17456 17472 17488 17504 17520 17536 17552 17568 17584 17600\n",
            "17616 17632 17648 17664 17680 17696 17712 17728 17744 17760 17776 17792 17808 17824 17840 17856 17872 17888 17904 17920\n",
            "17936 17952 17968 17984 18000 18016 18032 18048 18064 18080 18096 18112 18128 18144 18160 18176 18192 18208 18224 18240\n",
            "18256 18272 18288 18304 18320 18336 18352 18368 18384 18400 18416 18432 18448 18464 18480 18496 18512 18528 18544 18560\n",
            "18576 18592 18608 18624 18640 18656 18672 18688 18704 18720 18736 18752 18768 18784 18800 18816 18832 18848 18864 18880\n",
            "18896 18912 18928 18944 18960 18976 18992 19008 19024 19040 19056 19072 19088 19104 19120 19136 19152 19168 19184 19200\n",
            "19216 19232 19248 19264 19280 19296 19312 19328 19344 19360 19376 19392 19408 19424 19440 19456 19472 19488 19504 19520\n",
            "19536 19552 19568 19584 19600 19616 19632 19648 19664 19680 19696 19712 19728 19744 19760 19776 19792 19808 19824 19840\n",
            "19856 19872 19888 19904 19920 19936 19952 19968 19984 20000 20016 20032 20048 20064 20080 20096 20112 20128 20144 20160\n",
            "20176 20192 20208 20224 20240 20256 20272 20288 20304 20320 20336 20352 20368 20384 20400 20416 20432 20448 20464 20480\n",
            "20496 20512 20528 20544 20560 20576 20592 20608 20624 20640 20656 20672 20688 20704 20720 20736 20752 20768 20784 20800\n",
            "20816 20832 20848 20864 20880 20896 20912 20928 20944 20960 20976 20992 21008 21024 21040 21056 21072 21088 21104 21120\n",
            "21136 21152 21168 21184 21200 21216 21232 21248 21264 21280 21296 21312 21328 21344 21360 21376 21392 21408 21424 21440\n",
            "21456 21472 21488 21504 21520 21536 21552 21568 21584 21600 21616 21632 21648 21664 21680 21696 21712 21728 21744 21760\n",
            "21776 21792 21808 21824 21840 21856 21872 21888 21904 21920 21936 21952 21968 21984 22000 22016 22032 22048 22064 22080\n",
            "22096 22112 22128 22144 22160 22176 22192 22208 22224 22240 22256 22272 22288 22304 22320 22336 22352 22368 22384 22400\n",
            "22416 22432 22448 22464 22480 22496 22512 22528 22544 22560 22576 22592 22608 22624 22640 22656 22672 22688 22704 22720\n",
            "22736 22752 22768 22784 22800 22816 22832 22848 22864 22880 22896 22912 22928 22944 22960 22976 22992 23008 23024 23040\n",
            "23056 23072 23088 23104 23120 23136 23152 23168 23184 23200 23216 23232 23248 23264 23280 23296 23312 23328 23344 23360\n",
            "23376 23392 23408 23424 23440 23456 23472 23488 23504 23520 23536 23552 23568 23584 23600 23616 23632 23648 23664 23680\n",
            "23696 23712 23728 23744 23760 23776 23792 23808 23824 23840 23856 23872 23888 23904 23920 23936 23952 23968 23984 24000\n",
            "24016 24032 24048 24064 24080 24096 24112 24128 24144 24160 24176 24192 24208 24224 24240 24256 24272 24288 24304 24320\n",
            "24336 24352 24368 24384 24400 24416 24432 24448 24464 24480 24496 24512 24528 24544 24560 24576 24592 24608 24624 24640\n",
            "24656 24672 24688 24704 24720 24736 24752 24768 24784 24800 24816 24832 24848 24864 24880 24896 24912 24928 24944 24960\n",
            "24976 24992 25008 25024 25040 25056 25072 25088 25104 25120 25136 25152 25168 25184 25200 25216 25232 25248 25264 25280\n",
            "25296 25312 25328 25344 25360 25376 25392 25408 25424 25440 25456 25472 25488 25504 25520 25536 25552 25568 25584 25600\n",
            "25616 25632 25648 25664 25680 25696 25712 25728 25744 25760 25776 25792 25808 25824 25840 25856 25872 25888 25904 25920\n",
            "25936 25952 25968 25984 26000 26016 26032 26048 26064 26080 26096 26112 26128 26144 26160 26176 26192 26208 26224 26240\n",
            "26256 26272 26288 26304 26320 26336 26352 26368 26384 26400 26416 26432 26448 26464 26480 26496 26512 26528 26544 26560\n",
            "26576 26592 26608 26624 26640 26656 26672 26688 26704 26720 26736 26752 26768 26784 26800 26816 26832 26848 26864 26880\n",
            "26896 26912 26928 26944 26960 26976 26992 27008 27024 27040 27056 27072 27088 27104 27120 27136 27152 27168 27184 27200\n",
            "27216 27232 27248 27264 27280 27296 27312 27328 27344 27360 27376 27392 27408 27424 27440 27456 27472 27488 27504 27520\n",
            "27536 27552 27568 27584 27600 27616 27632 27648 27664 27680 27696 27712 27728 27744 27760 27776 27792 27808 27824 27840\n",
            "27856 27872 27888 27904 27920 27936 27952 27968 27984 28000 28016 28032 28048 28064 28080 28096 28112 28128 28144 28160\n",
            "28176 28192 28208 28224 28240 28256 28272 28288 28304 28320 28336 28352 28368 28384 28400 28416 28432 28448 28464 28480\n",
            "28496 28512 28528 28544 28560 28576 28592 28608 28624 28640 28656 28672 28688 28704 28720 28736 28752 28768 28784 28800\n",
            "28816 28832 28848 28864 28880 28896 28912 28928 28944 28960 28976 28992 29008 29024 29040 29056 29072 29088 29104 29120\n",
            "29136 29152 29168 29184 29200 29216 29232 29248 29264 29280 29296 29312 29328 29344 29360 29376 29392 29408 29424 29440\n",
            "29456 29472 29488 29504 29520 29536 29552 29568 29584 29600 29616 29632 29648 29664 29680 29696 29712 29728 29744 29760\n",
            "29776 29792 29808 29824 29840 29856 29872 29888 29904 29920 29936 29952 29968 29984 30000 30016 30032 30048 30064 30080\n",
            "30096 30112 30128 30144 30160 30176 30192 30208 30224 30240 30256 30272 30288 30304 30320 30336 30352 30368 30384 30400\n",
            "30416 30432 30448 30464 30480 30496 30512 30528 30544 30560 30576 30592 30608 30624 30640 30656 30672 30688 30704 30720\n",
            "30736 30752 30768 30784 30800 30816 30832 30848 30864 30880 30896 30912 30928 30944 30960 30976 30992 31008 31024 31040\n",
            "31056 31072 31088 31104 31120 31136 31152 31168 31184 31200 31216 31232 31248 31264 31280 31296 31312 31328 31344 31360\n",
            "31376 31392 31408 31424 31440 31456 31472 31488 31504 31520 31536 31552 31568 31584 31600 31616 31632 31648 31664 31680\n",
            "31696 31712 31728 31744 31760 31776 31792 31808 31824 31840 31856 31872 31888 31904 31920 31936 31952 31968 31984 32000\n",
            "32016 32032 32048 32064 32080 32096 32112 32128 32144 32160 32176 32192 32208 32224 32240 32256 32272 32288 32304 32320\n",
            "32336 32352 32368 32384 32400 32416 32432 32448 32464 32480 32496 32512 32528 32544 32560 32576 32592 32608 32624 32640\n",
            "32656 32672 32688 32704 32720 32736 32752 32768 32784 32800 32816 32832 32848 32864 32880 32896 32912 32928 32944 32960\n",
            "32976 32992 33008 33024 33040 33056 33072 33088 33104 33120 33136 33152 33168 33184 33200 33216 33232 33248 33264 33280\n",
            "33296 33312 33328 33344 33360 33376 33392 33408 33424 33440 33456 33472 33488 33504 33520 33536 33552 33568 33584 33600\n",
            "33616 33632 33648 33664 33680 33696 33712 33728 33744 33760 33776 33792 33808 33824 33840 33856 33872 33888 33904 33920\n",
            "33936 33952 33968 33984 34000 34016 34032 34048 34064 34080 34096 34112 34128 34144 34160 34176 34192 34208 34224 34240\n",
            "34256 34272 34288 34304 34320 34336 34352 34368 34384 34400 34416 34432 34448 34464 34480 34496 34512 34528 34544 34560\n",
            "34576 34592 34608 34624 34640 34656 34672 34688 34704 34720 34736 34752 34768 34784 34800 34816 34832 34848 34864 34880\n",
            "34896 34912 34928 34944 34960 34976 34992 35008 35024 35040 35056 35072 35088 35104 35120 35136 35152 35168 35184 35200\n",
            "35216 35232 35248 35264 35280 35296 35312 35328 35344 35360 35376 35392 35408 35424 35440 35456 35472 35488 35504 35520\n",
            "35536 35552 35568 35584 35600 35616 35632 35648 35664 35680 35696 35712 35728 35744 35760 35776 35792 35808 35824 35840\n",
            "35856 35872 35888 35904 35920 35936 35952 35968 35984 36000 36016 36032 36048 36064 36080 36096 36112 36128 36144 36160\n",
            "36176 36192 36208 36224 36240 36256 36272 36288 36304 36320 36336 36352 36368 36384 36400 36416 36432 36448 36464 36480\n",
            "36496 36512 36528 36544 36560 36576 36592 36608 36624 36640 36656 36672 36688 36704 36720 36736 36752 36768 36784 36800\n",
            "36816 36832 36848 36864 36880 36896 36912 36928 36944 36960 36976 36992 37008 37024 37040 37056 37072 37088 37104 37120\n",
            "37136 37152 37168 37184 37200 37216 37232 37248 37264 37280 37296 37312 37328 37344 37360 37376 37392 37408 37424 37440\n",
            "37456 37472 37488 37504 37520 37536 37552 37568 37584 37600 37616 37632 37648 37664 37680 37696 37712 37728 37744 37760\n",
            "37776 37792 37808 37824 37840 37856 37872 37888 37904 37920 37936 37952 37968 37984 38000 38016 38032 38048 38064 38080\n",
            "38096 38112 38128 38144 38160 38176 38192 38208 38224 38240 38256 38272 38288 38304 38320 38336 38352 38368 38384 38400\n",
            "38416 38432 38448 38464 38480 38496 38512 38528 38544 38560 38576 38592 38608 38624 38640 38656 38672 38688 38704 38720\n",
            "38736 38752 38768 38784 38800 38816 38832 38848 38864 38880 38896 38912 38928 38944 38960 38976 38992 39008 39024 39040\n",
            "39056 39072 39088 39104 39120 39136 39152 39168 39184 39200 39216 39232 39248 39264 39280 39296 39312 39328 39344 39360\n",
            "39376 39392 39408 39424 39440 39456 39472 39488 39504 39520 39536 39552 39568 39584 39600 39616 39632 39648 39664 39680\n",
            "39696 39712 39728 39744 39760 39776 39792 39808 39824 39840 39856 39872 39888 39904 39920 39936 39952 39968 39984 40000\n",
            "40016 40032 40048 40064 40080 40096 40112 40128 40144 40160 40176 40192 40208 40224 40240 40256 40272 40288 40304 40320\n",
            "40336 40352 40368 40384 40400 40416 40432 40448 40464 40480 40496 40512 40528 40544 40560 40576 40592 40608 40624 40640\n",
            "40656 40672 40688 40704 40720 40736 40752 40768 40784 40800 40816 40832 40848 40864 40880 40896 40912 40928 40944 40960\n",
            "40976 40992 41008 41024 41040 41056 41072 41088 41104 41120 41136 41152 41168 41184 41200 41216 41232 41248 41264 41280\n",
            "41296 41312 41328 41344 41360 41376 41392 41408 41424 41440 41456 41472 41488 41504 41520 41536 41552 41568 41584 41600\n",
            "41616 41632 41648 41664 41680 41696 41712 41728 41744 41760 41776 41792 41808 41824 41840 41856 41872 41888 41904 41920\n",
            "41936 41952 41968 41984 42000 42016 42032 42048 42064 42080 42096 42112 42128 42144 42160 42176 42192 42208 42224 42240\n",
            "42256 42272 42288 42304 42320 42336 42352 42368 42384 42400 42416 42432 42448 42464 42480 42496 42512 42528 42544 42560\n",
            "42576 42592 42608 42624 42640 42656 42672 42688 42704 42720 42736 42752 42768 42784 42800 42816 42832 42848 42864 42880\n",
            "42896 42912 42928 42944 42960 42976 42992 43008 43024 43040 43056 43072 43088 43104 43120 43136 43152 43168 43184 43200\n",
            "43216 43232 43248 43264 43280 43296 43312 43328 43344 43360 43376 43392 43408 43424 43440 43456 43472 43488 43504 43520\n",
            "43536 43552 43568 43584 43600 43616 43632 43648 43664 43680 43696 43712 43728 43744 43760 43776 43792 43808 43824 43840\n",
            "43856 43872 43888 43904 43920 43936 43952 43968 43984 44000 44016 44032 44048 44064 44080 44096 44112 44128 44144 44160\n",
            "44176 44192 44208 44224 44240 44256 44272 44288 44304 44320 44336 44352 44368 44384 44400 44416 44432 44448 44464 44480\n",
            "44496 44512 44528 44544 44560 44576 44592 44608 44624 44640 44656 44672 44688 44704 44720 44736 44752 44768 44784 44800\n",
            "44816 44832 44848 44864 44880 44896 44912 44928 44944 44960 44976 44992 45008 45024 45040 45056 45072 45088 45104 45120\n",
            "45136 45152 45168 45184 45200 45216 45232 45248 45264 45280 45296 45312 45328 45344 45360 45376 45392 45408 45424 45440\n",
            "45456 45472 45488 45504 45520 45536 45552 45568 45584 45600 45616 45632 45648 45664 45680 45696 45712 45728 45744 45760\n",
            "45776 45792 45808 45824 45840 45856 45872 45888 45904 45920 45936 45952 45968 45984 46000 46016 46032 46048 46064 46080\n",
            "46096 46112 46128 46144 46160 46176 46192 46208 46224 46240 46256 46272 46288 46304 46320 46336 46352 46368 46384 46400\n",
            "46416 46432 46448 46464 46480 46496 46512 46528 46544 46560 46576 46592 46608 46624 46640 46656 46672 46688 46704 46720\n",
            "46736 46752 46768 46784 46800 46816 46832 46848 46864 46880 46896 46912 46928 46944 46960 46976 46992 47008 47024 47040\n",
            "47056 47072 47088 47104 47120 47136 47152 47168 47184 47200 47216 47232 47248 47264 47280 47296 47312 47328 47344 47360\n",
            "47376 47392 47408 47424 47440 47456 47472 47488 47504 47520 47536 47552 47568 47584 47600 47616 47632 47648 47664 47680\n",
            "47696 47712 47728 47744 47760 47776 47792 47808 47824 47840 47856 47872 47888 47904 47920 47936 47952 47968 47984 48000\n",
            "48016 48032 48048 48064 48080 48096 48112 48128 48144 48160 48176 48192 48208 48224 48240 48256 48272 48288 48304 48320\n",
            "48336 48352 48368 48384 48400 48416 48432 48448 48464 48480 48496 48512 48528 48544 48560 48576 48592 48608 48624 48640\n",
            "48656 48672 48688 48704 48720 48736 48752 48768 48784 48800 48816 48832 48848 48864 48880 48896 48912 48928 48944 48960\n",
            "48976 48992 49008 49024 49040 49056 49072 49088 49104 49120 49136 49152 49168 49184 49200 49216 49232 49248 49264 49280\n",
            "49296 49312 49328 49344 49360 49376 49392 49408 49424 49440 49456 49472 49488 49504 49520 49536 49552 49568 49584 49600\n",
            "49616 49632 49648 49664 49680 49696 49712 49728 49744 49760 49776 49792 49808 49824 49840 49856 49872 49888 49904 49920\n",
            "49936 49952 \n",
            "    0    16    32    48    64    80    96   112   128   144   160   176   192   208   224   240   256   272   288   304   320\n",
            "  336   352   368   384   400   416   432   448   464   480   496   512   528   544   560   576   592   608   624   640\n",
            "  656   672   688   704   720   736   752   768   784   800   816   832   848   864   880   896   912   928   944   960\n",
            "  976   992  1008  1024  1040  1056  1072  1088  1104  1120  1136  1152  1168  1184  1200  1216  1232  1248  1264  1280\n",
            " 1296  1312  1328  1344  1360  1376  1392  1408  1424  1440  1456  1472  1488  1504  1520  1536  1552  1568  1584  1600\n",
            " 1616  1632  1648  1664  1680  1696  1712  1728  1744  1760  1776  1792  1808  1824  1840  1856  1872  1888  1904  1920\n",
            " 1936  1952  1968  1984  2000  2016  2032  2048  2064  2080  2096  2112  2128  2144  2160  2176  2192  2208  2224  2240\n",
            " 2256  2272  2288  2304  2320  2336  2352  2368  2384  2400  2416  2432  2448  2464  2480  2496  2512  2528  2544  2560\n",
            " 2576  2592  2608  2624  2640  2656  2672  2688  2704  2720  2736  2752  2768  2784  2800  2816  2832  2848  2864  2880\n",
            " 2896  2912  2928  2944  2960  2976  2992  3008  3024  3040  3056  3072  3088  3104  3120  3136  3152  3168  3184  3200\n",
            " 3216  3232  3248  3264  3280  3296  3312  3328  3344  3360  3376  3392  3408  3424  3440  3456  3472  3488  3504  3520\n",
            " 3536  3552  3568  3584  3600  3616  3632  3648  3664  3680  3696  3712  3728  3744  3760  3776  3792  3808  3824  3840\n",
            " 3856  3872 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZyghXul69Ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd7ef87-0f91-4e16-b33b-af21dbd17e91"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Flatten, InputLayer, BatchNormalization, Dropout\r\n",
        "import tensorflow.keras.callbacks as clb\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras import backend as K \r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "\r\n",
        "def build_model():\r\n",
        "    model = Sequential()\r\n",
        "    \r\n",
        "    model.add(InputLayer(input_shape=e_net_out_shape))\r\n",
        "    model.add(Flatten())\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(659, activation='relu'))\r\n",
        "    model.add(BatchNormalization())\r\n",
        "    model.add(Dropout(0.3))\r\n",
        "    model.add(Dense(39, activation='softmax'))\r\n",
        "\r\n",
        "    model.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "    return model\r\n",
        "\r\n",
        "def np_array_memmap_gen(feat_path, label_path, batch_size=128, shuffle_array=True):\r\n",
        "    while 1:\r\n",
        "        x = np.load(feat_path, mmap_mode='r')\r\n",
        "        y = np.load(label_path, mmap_mode='r')\r\n",
        "        lst = [i for i in range(x.shape[0])]\r\n",
        "\r\n",
        "        if shuffle_array:\r\n",
        "            random.shuffle(lst)\r\n",
        "\r\n",
        "        iters = len(lst) // batch_size + 1\r\n",
        "\r\n",
        "        for i in range(iters):\r\n",
        "            start = i * batch_size\r\n",
        "            end = (i + 1) * batch_size\r\n",
        "            yield (x[lst[start : end]], y[lst[start : end]])\r\n",
        "\r\n",
        "callbacks = [\r\n",
        "            clb.ReduceLROnPlateau(monitor='val_acc', factor=0.1, min_lr=1e-7, patience=2, verbose=1),\r\n",
        "            clb.EarlyStopping(monitor='val_acc', patience=4, verbose=1),\r\n",
        "            clb.ModelCheckpoint(monitor='val_acc', filepath='/content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5',\r\n",
        "                                save_best_only=True, verbose=1)\r\n",
        "            ]\r\n",
        "\r\n",
        "train_gen = np_array_memmap_gen('imgs.npy', 'labels.npy', batch_size=batch_size)\r\n",
        "val_gen = np_array_memmap_gen('val_imgs.npy', 'val_labels.npy', batch_size=batch_size)\r\n",
        "\r\n",
        "train_steps = nr_of_imgs // batch_size + 1\r\n",
        "val_steps = nr_of_val_imgs // batch_size + 1\r\n",
        "\r\n",
        "model = build_model()\r\n",
        "\r\n",
        "history = model.fit(train_gen, epochs=50, validation_data=val_gen, callbacks=callbacks, verbose=1, \r\n",
        "                    steps_per_epoch=train_steps, validation_steps=val_steps)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "781/781 [==============================] - 602s 771ms/step - loss: 2.8485 - acc: 0.2791 - val_loss: 2.7997 - val_acc: 0.2885\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.28845, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 2/50\n",
            "781/781 [==============================] - 637s 816ms/step - loss: 1.9238 - acc: 0.4403 - val_loss: 6.1224 - val_acc: 0.1626\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.28845\n",
            "Epoch 3/50\n",
            "781/781 [==============================] - 627s 803ms/step - loss: 1.7978 - acc: 0.4738 - val_loss: 2.7276 - val_acc: 0.4039\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.28845 to 0.40394, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 4/50\n",
            "781/781 [==============================] - 636s 816ms/step - loss: 1.6727 - acc: 0.5076 - val_loss: 2.2529 - val_acc: 0.4068\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.40394 to 0.40678, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 5/50\n",
            "781/781 [==============================] - 632s 811ms/step - loss: 1.6282 - acc: 0.5218 - val_loss: 2.6213 - val_acc: 0.3944\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.40678\n",
            "Epoch 6/50\n",
            "781/781 [==============================] - 622s 798ms/step - loss: 1.5648 - acc: 0.5374 - val_loss: 1.9251 - val_acc: 0.4770\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.40678 to 0.47695, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 7/50\n",
            "781/781 [==============================] - 630s 807ms/step - loss: 1.5121 - acc: 0.5577 - val_loss: 3.1295 - val_acc: 0.2794\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.47695\n",
            "Epoch 8/50\n",
            "781/781 [==============================] - 622s 797ms/step - loss: 1.4729 - acc: 0.5613 - val_loss: 1.5288 - val_acc: 0.5474\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.47695 to 0.54738, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 9/50\n",
            "781/781 [==============================] - 623s 799ms/step - loss: 1.4655 - acc: 0.5624 - val_loss: 1.3412 - val_acc: 0.6051\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.54738 to 0.60513, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 10/50\n",
            "781/781 [==============================] - 629s 807ms/step - loss: 1.4791 - acc: 0.5697 - val_loss: 5.4613 - val_acc: 0.1846\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.60513\n",
            "Epoch 11/50\n",
            "781/781 [==============================] - 624s 800ms/step - loss: 1.4534 - acc: 0.5709 - val_loss: 1.9660 - val_acc: 0.4065\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.60513\n",
            "Epoch 12/50\n",
            "781/781 [==============================] - 622s 798ms/step - loss: 1.3985 - acc: 0.5925 - val_loss: 1.2463 - val_acc: 0.7017\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.60513 to 0.70171, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 13/50\n",
            "781/781 [==============================] - 630s 807ms/step - loss: 1.3569 - acc: 0.6002 - val_loss: 1.0854 - val_acc: 0.6916\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.70171\n",
            "Epoch 14/50\n",
            "781/781 [==============================] - 624s 800ms/step - loss: 1.3162 - acc: 0.6096 - val_loss: 1.0715 - val_acc: 0.7053\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.70171 to 0.70533, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 15/50\n",
            "781/781 [==============================] - 633s 812ms/step - loss: 1.2809 - acc: 0.6215 - val_loss: 1.0760 - val_acc: 0.7053\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.70533\n",
            "Epoch 16/50\n",
            "781/781 [==============================] - 627s 804ms/step - loss: 1.2612 - acc: 0.6270 - val_loss: 1.0885 - val_acc: 0.7147\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.70533 to 0.71466, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 17/50\n",
            "781/781 [==============================] - 631s 809ms/step - loss: 1.2511 - acc: 0.6272 - val_loss: 1.3316 - val_acc: 0.6994\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.71466\n",
            "Epoch 18/50\n",
            "781/781 [==============================] - 626s 802ms/step - loss: 1.2319 - acc: 0.6326 - val_loss: 1.2768 - val_acc: 0.7172\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.71466 to 0.71724, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 19/50\n",
            "781/781 [==============================] - 635s 814ms/step - loss: 1.2113 - acc: 0.6406 - val_loss: 1.3407 - val_acc: 0.7183\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.71724 to 0.71828, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 20/50\n",
            "781/781 [==============================] - 626s 803ms/step - loss: 1.1816 - acc: 0.6493 - val_loss: 1.2675 - val_acc: 0.7069\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.71828\n",
            "Epoch 21/50\n",
            "781/781 [==============================] - 628s 805ms/step - loss: 1.1897 - acc: 0.6473 - val_loss: 1.3041 - val_acc: 0.7167\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.71828\n",
            "Epoch 22/50\n",
            "781/781 [==============================] - 625s 802ms/step - loss: 1.1543 - acc: 0.6568 - val_loss: 1.1870 - val_acc: 0.7253\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.71828 to 0.72527, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 23/50\n",
            "781/781 [==============================] - 634s 812ms/step - loss: 1.1673 - acc: 0.6567 - val_loss: 0.9475 - val_acc: 0.7271\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.72527 to 0.72708, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 24/50\n",
            "781/781 [==============================] - 639s 820ms/step - loss: 1.1445 - acc: 0.6632 - val_loss: 1.1064 - val_acc: 0.7222\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.72708\n",
            "Epoch 25/50\n",
            "781/781 [==============================] - 627s 804ms/step - loss: 1.1524 - acc: 0.6590 - val_loss: 1.1174 - val_acc: 0.7281\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.72708 to 0.72812, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 26/50\n",
            "781/781 [==============================] - 636s 815ms/step - loss: 1.1339 - acc: 0.6632 - val_loss: 1.1261 - val_acc: 0.7255\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.72812\n",
            "Epoch 27/50\n",
            "781/781 [==============================] - 624s 799ms/step - loss: 1.1401 - acc: 0.6575 - val_loss: 1.1827 - val_acc: 0.7206\n",
            "\n",
            "Epoch 00027: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.72812\n",
            "Epoch 28/50\n",
            "781/781 [==============================] - 625s 802ms/step - loss: 1.1303 - acc: 0.6639 - val_loss: 1.0580 - val_acc: 0.7320\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.72812 to 0.73200, saving model to /content/drive/My Drive/PDR/Results/models/ClassifierB0_2Layers.h5\n",
            "Epoch 29/50\n",
            "781/781 [==============================] - 640s 821ms/step - loss: 1.1301 - acc: 0.6615 - val_loss: 1.0476 - val_acc: 0.7229\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.73200\n",
            "Epoch 30/50\n",
            "781/781 [==============================] - 629s 806ms/step - loss: 1.1231 - acc: 0.6630 - val_loss: 1.1539 - val_acc: 0.7307\n",
            "\n",
            "Epoch 00030: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.73200\n",
            "Epoch 31/50\n",
            "781/781 [==============================] - 625s 801ms/step - loss: 1.1325 - acc: 0.6655 - val_loss: 0.9373 - val_acc: 0.7240\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.73200\n",
            "Epoch 32/50\n",
            "781/781 [==============================] - 629s 807ms/step - loss: 1.1273 - acc: 0.6649 - val_loss: 1.3669 - val_acc: 0.7281\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1e-07.\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.73200\n",
            "Epoch 00032: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}